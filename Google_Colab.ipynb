{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File containing code for utilisation of the project on google colab "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1- Add data in your google drive and link it to this notebook be running :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2- Installation of Julia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PMGwZ7aFJL8Y"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%%shell\n",
        "if ! command -v julia 3>&1 > /dev/null\n",
        "then\n",
        "    wget -q 'https://julialang-s3.julialang.org/bin/linux/x64/1.7/julia-1.7.2-linux-x86_64.tar.gz' \\\n",
        "        -O /tmp/julia.tar.gz\n",
        "    tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "    rm /tmp/julia.tar.gz\n",
        "fi\n",
        "julia -e 'using Pkg; pkg\"add IJulia; precompile;\"'\n",
        "echo 'Done'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Need to *Save* setting julia :\n",
        "- Edit\n",
        "- Notebook settings\n",
        "- Verify that runtime type = julia 1.7\n",
        "- Save\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3- Download Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "using Pkg;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import Pkg; Pkg.add(\"DataFrames\")\n",
        "import Pkg; Pkg.add(\"MLJ\")\n",
        "import Pkg; Pkg.add(\"MLJLinearModels\")\n",
        "import Pkg; Pkg.add(\"CSV\")\n",
        "import Pkg; Pkg.add(\"Random\")\n",
        "import Pkg; Pkg.add(\"CUDA\")\n",
        "import Pkg; Pkg.add(\"MLJMultivariateStatsInterface\")\n",
        "import Pkg; Pkg.add(\"Flux\")\n",
        "import Pkg; Pkg.add(\"MLJFlux\")\n",
        "import Pkg; Pkg.add(\"ComputationalResources\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "using DataFrames, MLJFlux, CSV, MLJ,  MLJMultivariateStatsInterface, CUDA, Flux, Random, MLJLinearModels, ComputationalResources"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4- Add your functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "function fit_and_evaluate(training_data, validation_data, test_data,validation_test)\n",
        "    \"\"\"\n",
        "        fit the data with a multinomailClassifier and evaluate it.\n",
        "\n",
        "    Arguments:\n",
        "        training_data {DataFrame} -- training data\n",
        "        valididation_data {DataFrame} -- training labels\n",
        "        test_data {DataFrame} -- test data\n",
        "        validation_test {DataFrame} -- test label\n",
        "\n",
        "    Returns:\n",
        "        mach {machine} -- trained machine\n",
        "        error {DataFrame} -- dataframe of training and test error\n",
        "\n",
        "    \"\"\"\n",
        "    mach = machine(MultinomialClassifier(penalty = :l1, lambda =1e-7), training_data, validation_data)|> gpu |> fit!\n",
        "    #mach = machine(MultinomialClassifier(penalty = :none), training_data, validation_data)|> gpu |> fit!\n",
        "    error = DataFrame(trainin_error = mean(predict_mode(mach, training_data) .!= validation_data), test_error = mean(predict_mode(mach, test_data) .!= validation_test))\n",
        "    return mach, error\n",
        "end\n",
        "\n",
        "function data_split(data,y, idx_train, idx_test; shuffle =true)\n",
        "    \"\"\"\n",
        "        Split data between a train and test set\n",
        "\n",
        "    Arguments:\n",
        "        data {DataFrame} -- all the data to split\n",
        "        y {DataFrame} -- labels of the data\n",
        "        idx_train {UnitRange{Int64}} -- indexes of train data\n",
        "        idx_test {UnitRange{Int64}} -- indexes of test data\n",
        "        shuffle {boolean} -- if true shuffle the data\n",
        "\n",
        "    Returns:\n",
        "        train {DataFrame} -- training data\n",
        "        train_valid {DataFrame} -- training labels\n",
        "        test {DataFrame} -- test data\n",
        "        test_valid {DataFrame} -- test label\n",
        "\n",
        "    \"\"\"\n",
        "    if shuffle\n",
        "        idxs = randperm(size(data, 1))\n",
        "    else\n",
        "        idxs= 1:size(data, 1)\n",
        "    end\n",
        "    return (train = data[idxs[idx_train], :],\n",
        "    train_valid = y[idxs[idx_train], 1],\n",
        "    test = data[idxs[idx_test], :],\n",
        "    test_valid = y[idxs[idx_test], 1])\n",
        "    end\n",
        "\n",
        "function multinom_class(x_train, x_test, y)\n",
        "    \"\"\"\n",
        "        Multinomial Classification. Save the prediction in a csv file.\n",
        "\n",
        "    Arguments:\n",
        "        x_train {DataFrame} -- train set (without labels)\n",
        "        x_test {DataFrame} -- test set to predict\n",
        "        y {DataFrame} -- labels of the training data\n",
        "        pena {} -- penalty\n",
        "\n",
        "    Returns:\n",
        "        mach{} -- machine\n",
        "    \"\"\"\n",
        "    #mach = machine(MultinomialClassifier(penalty = pena, lambda = lambda), x_train, y) |> fit!\n",
        "    mach = machine(MultinomialClassifier(penalty = :l2, lambda =1e-3), x_train, y)|> gpu |> fit!\n",
        "    pred = predict_mode(mach, x_test)\n",
        "    println(pred)\n",
        "    kaggle_submit(pred, \"RidgeClassifier_pca_8000\")\n",
        "    return mach\n",
        "end\n",
        "\n",
        "\n",
        "function lasso_classifier(x_train, x_test, y;seed=0, goal, lower, upper)\n",
        "    \"\"\"\n",
        "        Lasso Classification using cross-validation. Save the prediction in a csv file.\n",
        "\n",
        "    Arguments:\n",
        "        x_train {DataFrame} -- train set (without labels)\n",
        "        x_test {DataFrame} -- test set to predict\n",
        "        y {DataFrame} -- labels of the training data\n",
        "        seed {int} -- value of the seed to fix\n",
        "        goal {int} -- number of different lambda to try\n",
        "        lower {float} -- value of the smallest lambda to try\n",
        "        upper {float} -- value of the biggest lambda to try\n",
        "\n",
        "    Returns:\n",
        "        mach{} -- machine\n",
        "\n",
        "    \"\"\"\n",
        "    Random.seed!(seed)\n",
        "    model = MultinomialClassifier(penalty = :l1)\n",
        "    mach_lasso = machine(TunedModel(model = model,\n",
        "                                    resampling = CV(nfolds = 5),\n",
        "                                    tuning = Grid(goal = goal),\n",
        "                                    range = range(model, :lambda, lower = lower, upper = upper, scale = :log10),\n",
        "                                    measure = MisclassificationRate()),\n",
        "                                    x_train, y) |> gpu |>fit!\n",
        "    pred = predict_mode(mach_lasso, x_test)\n",
        "    kaggle_submit(pred, \"LassoClassifier_6_12\")  \n",
        "    return mach_lasso\n",
        "end\n",
        "\n",
        "function ridge_classifier(x_train, x_test, y; seed=0, goal, lower, upper)\n",
        "    \"\"\"\n",
        "        Ridge Classification using cross-validation. Save the prediction in a csv file.\n",
        "\n",
        "    Arguments:\n",
        "        x_train {DataFrame} -- train set (without labels)\n",
        "        x_test {DataFrame} -- test set to predict\n",
        "        y {DataFrame} -- labels of the training data\n",
        "        seed {int} -- value of the seed to fix\n",
        "        goal {int} -- number of different lambda to try\n",
        "        lower {float} -- value of the smallest lambda to try\n",
        "        upper {float} -- value of the biggest lambda to try\n",
        "\n",
        "    Returns:\n",
        "        mach{} -- machine\n",
        "\n",
        "    \"\"\"\n",
        "    Random.seed!(seed)\n",
        "    model = MultinomialClassifier(penalty = :l2)\n",
        "    tuned_model_ridge = TunedModel(model = model,\n",
        "                                    resampling = CV(nfolds = 5),\n",
        "                                    tuning = Grid(goal = goal),\n",
        "                                    range = range(model, :lambda, lower = lower, upper = upper, scale = :log10),\n",
        "                                    measure = MisclassificationRate())\n",
        "    mach_ridge = machine(tuned_model_ridge, x_train, y) |> gpu |>fit!\n",
        "    pred = predict_mode(mach_ridge, x_test)\n",
        "    kaggle_submit(pred, \"RidgeClassifier_30_11\")  \n",
        "    return mach_ridge\n",
        "end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "function load_data(path)\n",
        "    \"\"\"\n",
        "        Load the data at \"path\" localization in the form of a dataframe\n",
        "\n",
        "    Arguments:\n",
        "        path {string} -- localization of the data to download\n",
        "\n",
        "    Returns :\n",
        "        Dataframe {DataFrame} -- DataFrame with the data\n",
        "    \"\"\"\n",
        "    return DataFrame(CSV.File(path))\n",
        "end\n",
        "\n",
        "function remove_constant_predictors(df)\n",
        "    \"\"\"\n",
        "        Remove constant columns in a given DataFrame\n",
        "\n",
        "    Arguments:\n",
        "        df {DataFrame} -- data to clean\n",
        "\n",
        "    Returns :\n",
        "        df_no_const {DataFrame} -- New DataFrame without constante columns/predictors\n",
        "    \"\"\"\n",
        "    df_no_const = df[:,  std.(eachcol(df)) .!= 0]\n",
        "    return df_no_const\n",
        "end\n",
        "\n",
        "function remove_prop_predictors(df)\n",
        "    \"\"\"\n",
        "        Remove exact correlated columns in a given DataFrame\n",
        "\n",
        "    Arguments:\n",
        "        df {DataFrame} -- data to clean\n",
        "\n",
        "    Returns :\n",
        "        df_no_const {DataFrame} -- New DataFrame without proportionnal columns/predictors\n",
        "    \"\"\"\n",
        "    corr_pairs = findall(≈(1), cor(Matrix(df))) |> idxs -> filter(x -> x[1] > x[2], idxs)\n",
        "    corr_pred = getindex.(corr_pairs,1)\n",
        "    corr_pred = unique(corr_pred)\n",
        "    return df[:,Not(corr_pred)]\n",
        "end\n",
        "\n",
        "function remove_low_call_rates(df)\n",
        "\n",
        "    rates = zeros(0)\n",
        "    for column in (eachcol(df))\n",
        "        append!( rates,(sum(x->x>0, column) /length(df[:,1])) *100)\n",
        "    end\n",
        "    call_rates = DataFrame(index = names(df) , rates = rates )\n",
        "\n",
        "    call_rates =call_rates[(call_rates.rates.>1),:]\n",
        "    df_clean = select(df, call_rates.index)\n",
        "    PlotlyJS.plot(call_rates, x=:rates, kind=\"histogram\", nbinsx=20', Layout(title_text=\"Histogram of call-rates\", xaxis_title_text=\"Pourcentage of call rates in all gene\", yaxis_title_text=\"Number of gene\"))\n",
        "    return df_clean\n",
        "end\n",
        "\n",
        "function coef_info(beta_df, x_train)\n",
        "    df = permutedims(beta_df, 1)\n",
        "    df.stds = std.(eachcol(x_train))\n",
        "    df.t_value = df[:,2] ./ df.stds\n",
        "    df.abs_t = abs.(df.t_value)\n",
        "    return df\n",
        "end\n",
        "\n",
        "function get_names_len(X, y, len)\n",
        "    mach = machine(MultinomialClassifier(penalty = :none), X, y)\n",
        "    fit!(mach, verbosity = 0)\n",
        "    params = fitted_params(mach)\n",
        "    df = hcat(DataFrame(titles = levels(params.classes)), DataFrame(params.coefs))\n",
        "    info = DataFrame(genes = names(df[:,2:end]))\n",
        "    for i in range(1,3,3)\n",
        "        #info.levels(params.classes)[i] = coef_info(DataFrame(df[i, :]), X)\n",
        "        info = hcat(info, coef_info(DataFrame(df[Int(i), :]), X).abs_t, makeunique=true)\n",
        "    end\n",
        "    info = permutedims(info, 1)\n",
        "    maxs = DataFrame(genes = names(info[:,2:end]) ,maxs = maximum.(eachcol(info[:,2:end])))\n",
        "    sort!(maxs, :maxs, rev = true)\n",
        "    #chosen_names = names(permutedims(maxs[maxs.maxs .> 1, :], 1)[:,2:end])\n",
        "    #maxs\n",
        "    return maxs[1:len,:].genes#names(permutedims(maxs[maxs.maxs .> cutoff, :], 1)[:,2:end])\n",
        "end\n",
        "\n",
        "function norm(x_train, x_test)\n",
        "    \"\"\"\n",
        "        Normalize the data. Compute the norm and apply it to the data with a MLJ transform\n",
        "\n",
        "    Arguments:\n",
        "        x_train {DataFrame} -- train set (without labels) to normalize\n",
        "        x_test {DataFrame} -- test set to normalize\n",
        "\n",
        "    Returns :\n",
        "        norm_data[1:5000,:] {DataFrame} -- Normalized train set (x_train)\n",
        "        norm_data[5001:end,:] {DataFrame} -- Normalized test set (x_test)\n",
        "    \"\"\"\n",
        "    total_data = vcat(x_train, x_test)\n",
        "    mach = fit!(machine(Standardizer(), total_data));\n",
        "    norm_data = MLJ.transform(mach, total_data)\n",
        "    return norm_data[1:5000,:], norm_data[5001:end,:]\n",
        "end\n",
        "\n",
        "function clean_data(train_df, test_df; normalised=false, from_index=true)\n",
        "    \"\"\"\n",
        "        Prepare the data by removing constant and correlated predictors. Can also normalized the data.\n",
        "\n",
        "    Arguments:\n",
        "        train_df {DataFrame} -- train set (with labels)\n",
        "        test_df {DataFrame} -- test set\n",
        "        normalised {Boolean} -- If true the function normalize train and test data.\n",
        "        from_index {Boolean} -- Determine the way to clean the data. If true, the function read a dataframe\n",
        "                                containing the indexes to keep. (This was previously done to gain time in the \n",
        "                                pre-processing of the data). If false use functions remove_prop_predictors and \n",
        "                                remove_constant_predictors to clean the data.\n",
        "\n",
        "    Returns :\n",
        "    x_train {DataFrame} -- cleaned train set \n",
        "    x_test {DataFrame} -- cleaned test set\n",
        "    y {DataFrame} -- labels of the training data\n",
        "    \"\"\"\n",
        "\n",
        "    if from_index\n",
        "        indexes = load_data(\"/content/gdrive/MyDrive/Colab Notebooks/MLProject/data/indexes_old.csv\") #indexes of the cleaned data, to gain time\n",
        "        x_train = select(train_df, indexes.index)\n",
        "        x_test = select(test_df, indexes.index)\n",
        "        y = coerce!(train_df, :labels => Multiclass).labels\n",
        "        if normalised\n",
        "            x_train, x_test = norm(x_train, x_test)\n",
        "        end\n",
        "\n",
        "    else\n",
        "        x_train = remove_constant_predictors(select(train_df, Not(:labels)))\n",
        "        x_test = remove_constant_predictors(select(test_df, names(x_train)))\n",
        "        x_train = select(train_df, names(x_test))\n",
        "\n",
        "        x_test = remove_prop_predictors(x_test)\n",
        "        x_train = remove_prop_predictors(select(train_df, names(x_test)))\n",
        "        x_test = select(test_df, names(x_train))\n",
        "\n",
        "        x_test = remove_low_call_rates(x_test)\n",
        "        x_train = remove_low_call_rates(select(train_df, names(x_test)))\n",
        "        x_test = select(test_df, names(x_train))\n",
        "\n",
        "        y = coerce!(train_df, :labels => Multiclass).labels\n",
        "\n",
        "        if normalised\n",
        "            x_train, x_test = norm(x_train, x_test)\n",
        "        end    \n",
        "\n",
        "        CSV.write(\"./data/indexes.csv\",DataFrame(index=names(x_train)))\n",
        "    end\n",
        "    return x_train,x_test,y\n",
        "end\n",
        "\n",
        "function kaggle_submit(df_prediction, title)\n",
        "    \"\"\"\n",
        "        Save a csv file for a kaggle submission with prediction for the test set\n",
        "\n",
        "    Arguments:\n",
        "        df_prediction {DataFrame} -- prediction for the test set\n",
        "        title {string} -- name of the file to save\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    prediction_kaggle = DataFrame(id = collect(1:length(df_prediction)))\n",
        "    prediction_kaggle[!,:prediction] = df_prediction\n",
        "    CSV.write(\"/content/gdrive/MyDrive/Colab Notebooks/MLProject/Submission/Submission_$(title).csv\", prediction_kaggle)\n",
        "end\n",
        "\n",
        "function pca(df,dimension)\n",
        "    \"\"\"\n",
        "        Do a pca \n",
        "\n",
        "    Arguments:\n",
        "        df {DataFrame} -- data on which to do the pca\n",
        "        dimension {int} -- dimension of the pca\n",
        "\n",
        "    Returns :\n",
        "        df_no_const {DataFrame} -- New DataFrame without proportionnal columns/predictors\n",
        "    \"\"\"\n",
        "    return MLJ.transform(fit!(machine(PCA(maxoutdim = dimension), df)), df)\n",
        "end\n",
        "\n",
        "function pca_cumvar_plot(training_data)\n",
        "    pca_gene = fit!(machine(PCA(), training_data), verbosity = 0);\n",
        "    vars = report(pca_gene).principalvars ./ report(pca_gene).tvar\n",
        "    return report(pca_gene).principalvars\n",
        "end\n",
        "\n",
        "function call_rates(df,pourcent)\n",
        "    \"\"\"\n",
        "        Return columns with low call rates in a given DataFrame. The call rate for a given gene is defined as the proportion of measurement\n",
        "        for which the corresponding gene information is not 0. We keep only gene whose call rate is > 1%\n",
        "\n",
        "    Arguments:\n",
        "        df {DataFrame} -- data to clean\n",
        "\n",
        "    Returns :\n",
        "        df_no_const {DataFrame} -- New DataFrame without proportionnal columns/predictors\n",
        "    \"\"\"\n",
        "    rates = zeros(0)\n",
        "    for column in (eachcol(df))\n",
        "        append!( rates,(sum(x->x>0, column) /length(df[:,1])) *100)\n",
        "    end\n",
        "    call_rates = DataFrame(index = names(df) , rates = rates)\n",
        "    call_rates = call_rates[(call_rates.rates.>pourcent),:]\n",
        "    return call_rates.index\n",
        "end"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5- Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = (DataFrame(CSV.File(\"/content/gdrive/MyDrive/Colab Notebooks/MLProject/data/train.csv.gz\"))) |> gpu #to use colab gpu\n",
        "test_df = (DataFrame(CSV.File(\"/content/gdrive/MyDrive/Colab Notebooks/MLProject/data/test.csv.gz\"))) |> gpu"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6- Run your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#clean data\n",
        "x_train,x_test,y = clean_data(train_df, test_df,normalised=true, from_index=true) |> gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Mean difference selection\n",
        "\n",
        "mean_CBP = mean.(eachcol(x_train[(y.==\"CBP\"),:]))\n",
        "mean_KAT5 = mean.(eachcol(x_train[(y.==\"KAT5\"),:]))\n",
        "mean_eGFP = mean.(eachcol(x_train[(y.==\"eGFP\"),:]))\n",
        "\n",
        "results_mean= DataFrame(gene = names(x_train), CBP= mean_CBP, KAT5= mean_KAT5, eGFP = mean_eGFP, diff1=abs.(mean_CBP-mean_eGFP), diff2=abs.(mean_eGFP -mean_KAT5), diff3=(abs.(mean_CBP -mean_KAT5)))\n",
        "\n",
        "sort!(results_mean, [:diff1], rev=true)\n",
        "selection1 = results_mean[1:6000,:] \n",
        "sort!(results_mean, [:diff2], rev=true)\n",
        "selection2 = results_mean[1:6000,:]\n",
        "sort!(results_mean, [:diff3], rev=true)\n",
        "selection3 = results_mean[1:6000,:]\n",
        "\n",
        "x_train2 = select(x_train, unique([selection1.gene; selection2.gene; selection3.gene]))\n",
        "\n",
        "x_train2 = MLJ.transform(fit!(machine(PCA(maxoutdim = 3000), x_train2)), x_train2)\n",
        "\n",
        "Random.seed!(0)\n",
        "\n",
        "x_test = select(x_test, names(x_train2))\n",
        "data = vcat(x_train2,x_test)\n",
        "# Do a PCA to reduce the features to 3000\n",
        "data = MLJ.transform(fit!(machine(PCA(maxoutdim = 3000), data)), data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train2= data[1:5000,:]\n",
        "x_test = data[5000:8093,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = NeuralNetworkClassifier( builder = MLJFlux.Short(n_hidden = 128,\n",
        "σ = relu, dropout = 0.5),\n",
        "optimiser = ADAM(),\n",
        "batch_size = 128,\n",
        "epochs = 2000,\n",
        "alpha = 0.25)\n",
        "\n",
        "mach = fit!(machine(model,x_train2, y), verbosity = 1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = predict_mode(mach, x_test)\n",
        "kaggle_submit(pred, \"NN_mean\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Julia_1.7.2_template.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Julia 1.7.3",
      "language": "julia",
      "name": "julia-1.7"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
